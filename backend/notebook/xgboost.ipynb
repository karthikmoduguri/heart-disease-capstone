{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a875263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Created new interaction features.\n",
      "\n",
      "Starting smarter hyperparameter tuning (RandomizedSearchCV)...\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Tuning complete.\n",
      "Best Parameters found: {'model__subsample': 0.9, 'model__reg_lambda': 0.5, 'model__reg_alpha': 1, 'model__n_estimators': 100, 'model__max_depth': 3, 'model__learning_rate': 0.01, 'model__gamma': 0, 'model__colsample_bytree': 0.8}\n",
      "Best Accuracy during tuning: 82.77%\n",
      "\n",
      "Evaluating the best model on the test set...\n",
      "Test Set Accuracy: 83.33%\n",
      "\n",
      "Classification Report on Test Set:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "No Disease (0)       0.79      0.94      0.86        32\n",
      "   Disease (1)       0.91      0.71      0.80        28\n",
      "\n",
      "      accuracy                           0.83        60\n",
      "     macro avg       0.85      0.83      0.83        60\n",
      "  weighted avg       0.85      0.83      0.83        60\n",
      "\n",
      "\n",
      "Model (full pipeline) saved successfully as 'xgboost_heart_pipeline_v2.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. Load Data\n",
    "try:\n",
    "    df = pd.read_csv('heart_cleveland_upload.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'heart_cleveland_upload.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 1: FEATURE ENGINEERING (New)\n",
    "# ----------------------------------------------------\n",
    "# Create some interaction features\n",
    "df['age_x_trestbps'] = df['age'] * df['trestbps']\n",
    "df['age_x_chol'] = df['age'] * df['chol']\n",
    "df['chol_x_trestbps'] = df['chol'] * df['trestbps']\n",
    "print(\"Created new interaction features.\")\n",
    "\n",
    "# 2. Define Features (X) and Target (y)\n",
    "X = df.drop('condition', axis=1)\n",
    "y = df['condition']\n",
    "\n",
    "# Identify which columns are continuous (need scaling) and which are not\n",
    "continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', \n",
    "                       'age_x_trestbps', 'age_x_chol', 'chol_x_trestbps']\n",
    "                       \n",
    "# All other columns are treated as categorical/binary\n",
    "categorical_features = [col for col in X.columns if col not in continuous_features]\n",
    "\n",
    "# 3. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 2: CREATE A PREPROCESSING & MODELING PIPELINE (New)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Create a transformer to scale continuous features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Keep the categorical columns as-is\n",
    ")\n",
    "\n",
    "# Create the full pipeline\n",
    "# 'preprocessor' is our scaling step\n",
    "# 'model' is our XGBoost classifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', xgb.XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# STEP 3: SMARTER HYPERPARAMETER TUNING (RandomizedSearchCV)\n",
    "# ----------------------------------------------------\n",
    "# We expand the grid to give the model more options to learn\n",
    "# Note: Parameter names must now start with 'model__' to tell the pipeline\n",
    "# which step to apply the parameter to.\n",
    "\n",
    "param_grid_random = {\n",
    "    # Try more trees and a wider range\n",
    "    'model__n_estimators': [100, 200, 300, 400, 500],\n",
    "    # More depth options\n",
    "    'model__max_depth': [3, 4, 5, 6, 7],\n",
    "    # Finer learning rate control\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "    'model__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'model__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    # Add regularization to prevent overfitting (important!)\n",
    "    'model__gamma': [0, 0.1, 0.5, 1],\n",
    "    'model__reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'model__reg_lambda': [0, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# n_iter=100 will try 100 different random combinations\n",
    "# cv=5 is 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid_random,\n",
    "    n_iter=100,  # Try 100 combinations (you can lower this if it's too slow)\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nStarting smarter hyperparameter tuning (RandomizedSearchCV)...\")\n",
    "# 5. Train the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(\"Tuning complete.\")\n",
    "print(f\"Best Parameters found: {random_search.best_params_}\")\n",
    "print(f\"Best Accuracy during tuning: {random_search.best_score_ * 100:.2f}%\")\n",
    "\n",
    "# 6. Evaluate the Best Model on the Test Set\n",
    "print(\"\\nEvaluating the best model on the test set...\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Set Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report on Test Set:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No Disease (0)', 'Disease (1)']))\n",
    "\n",
    "# 7. Save the Model\n",
    "# The 'best_model' is the entire pipeline (preprocessor + model)\n",
    "# This is perfect, because now it will automatically scale new data \n",
    "# before making a prediction.\n",
    "model_filename = 'xgboost_heart_pipeline_v2.pkl'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"\\nModel (full pipeline) saved successfully as '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d710cf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\karth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xgboost) (2.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\karth\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from xgboost) (1.15.3)\n",
      "Downloading xgboost-3.1.1-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.8/72.0 MB 12.6 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 5.0/72.0 MB 14.4 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 10.2/72.0 MB 17.7 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 15.7/72.0 MB 19.8 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 23.9/72.0 MB 23.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 31.2/72.0 MB 25.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 37.7/72.0 MB 26.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 46.7/72.0 MB 28.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 53.2/72.0 MB 29.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 60.8/72.0 MB 29.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 68.7/72.0 MB 30.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 30.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 28.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
